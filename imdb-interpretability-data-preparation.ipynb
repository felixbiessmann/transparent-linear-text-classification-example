{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability for a Linear Text Classification Model\n",
    "\n",
    "In this example we show how to apply a simple white-box interpretability method used by [Haufe et al.](https://www.sciencedirect.com/science/article/pii/S1053811913010914) in neuroimaging applications to the predictions of a linear text classification model operating on unigram bag-of-words features. \n",
    "\n",
    "An empirical quantification of interpretability quality based on human-in-the-loop experiments by [Schmidt and Biessmann](https://arxiv.org/pdf/1901.08558.pdf) indicates that this method yields better explanations of machine learning (ML) predictions. Better here means that human annotators become faster and more accurate when assisted with these explanations, compared to explanations of other interpretability methods, such as [LIME](https://arxiv.org/pdf/1602.04938v1.pdf).\n",
    "\n",
    "This notebook shows how to train the ML model and interpretability model used in [the study by Schmidt and Biessmann](https://arxiv.org/pdf/1901.08558.pdf). The classification task is to predict whether the sentiment of an IMDB movie review was positive or negative, a standard text classification benchmark due to [Maas et al](http://www.aclweb.org/anthology/P11-1015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First, download the data behind [this link](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz), unzip/-tar it and adapt the path in the below code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# contains the unzipped data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "DATADIR = \"aclImdb\"\n",
    "\n",
    "top_what = 3\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "\n",
    "features = 'review'\n",
    "label = 'rating_binary'\n",
    "\n",
    "def load_data(path = DATADIR):\n",
    "    reviews = []\n",
    "    for train_test_split in ['train', 'test']:\n",
    "        for label in ['pos','neg']:\n",
    "            for file in glob.glob(os.path.join(DATADIR, train_test_split, label, '*.txt')):\n",
    "                reviews.append({\n",
    "                    'review': open(file, encoding='utf-8').read(),\n",
    "                    'movie_id': int(file.split(\"/\")[-1].split(\"_\")[0]),\n",
    "                    'rating_binary': label,\n",
    "                    'rating': int(file.split(\"/\")[-1].split('_')[1].split(\".\")[0]),\n",
    "                    'split': train_test_split\n",
    "                })\n",
    "    return pd.DataFrame(reviews)\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "df[features] = df[features].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "\n",
    "train_df = df[df.split=='train']\n",
    "test_df = df[df.split=='test']\n",
    "test_df.index = range(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating_binary</th>\n",
       "      <th>rating</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>4715</td>\n",
       "      <td>pos</td>\n",
       "      <td>9</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>12390</td>\n",
       "      <td>pos</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>8329</td>\n",
       "      <td>pos</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>9063</td>\n",
       "      <td>pos</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>3092</td>\n",
       "      <td>pos</td>\n",
       "      <td>10</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>With actors like Depardieu and Richard it is r...</td>\n",
       "      <td>11513</td>\n",
       "      <td>neg</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>If you like to get a couple of fleeting glimps...</td>\n",
       "      <td>5409</td>\n",
       "      <td>neg</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>When something can be anything you want it to ...</td>\n",
       "      <td>11187</td>\n",
       "      <td>neg</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I had heard good things about \"States of Grace...</td>\n",
       "      <td>9359</td>\n",
       "      <td>neg</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Well, this movie actually did have one redeemi...</td>\n",
       "      <td>11556</td>\n",
       "      <td>neg</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  movie_id  \\\n",
       "0      For a movie that gets no respect there sure ar...      4715   \n",
       "1      Bizarre horror movie filled with famous faces ...     12390   \n",
       "2      A solid, if unremarkable film. Matthau, as Ein...      8329   \n",
       "3      It's a strange feeling to sit alone in a theat...      9063   \n",
       "4      You probably all already know this by now, but...      3092   \n",
       "...                                                  ...       ...   \n",
       "49995  With actors like Depardieu and Richard it is r...     11513   \n",
       "49996  If you like to get a couple of fleeting glimps...      5409   \n",
       "49997  When something can be anything you want it to ...     11187   \n",
       "49998  I had heard good things about \"States of Grace...      9359   \n",
       "49999  Well, this movie actually did have one redeemi...     11556   \n",
       "\n",
       "      rating_binary  rating  split  \n",
       "0               pos       9  train  \n",
       "1               pos       8  train  \n",
       "2               pos       7  train  \n",
       "3               pos       8  train  \n",
       "4               pos      10  train  \n",
       "...             ...     ...    ...  \n",
       "49995           neg       1   test  \n",
       "49996           neg       1   test  \n",
       "49997           neg       1   test  \n",
       "49998           neg       3   test  \n",
       "49999           neg       1   test  \n",
       "\n",
       "[50000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a linear unigram bag-of-words classifier\n",
    "\n",
    "Given that we restrict the model to unigram features, the hyperparameters of the classifier  in the code are optimal, as determined in offline experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felix/anaconda3/envs/ppp/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/felix/anaconda3/envs/ppp/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.88      0.87      0.87     12500\n",
      "         pos       0.87      0.88      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a unigram BOW vectorizer\n",
    "vect = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS).fit(train_df[features])\n",
    "X_train = vect.transform(train_df[features])\n",
    "X_test = vect.transform(test_df[features])\n",
    "\n",
    "clf = SGDClassifier(loss='log', \n",
    "                    n_jobs=-1, \n",
    "                    random_state=random_state,\n",
    "                    alpha=0.0001\n",
    "                   ).fit(X_train, train_df[label])\n",
    "\n",
    "test_df['predictions'] = clf.predict(X_test)\n",
    "predictions_proba = clf.predict_proba(X_test)\n",
    "test_df['prediction_proba_max'] = predictions_proba.max(axis=1)\n",
    "\n",
    "print(classification_report(test_df[label], test_df['predictions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering the model interpretable\n",
    "\n",
    "As we only trained a linear text classification, there is a simple way of rendering the model and its predictions interpretable. \n",
    "\n",
    "Note that **we cannot simply interpret the coefficients of the linear model itself**! This would implicitly assume that the covariates (the features or data) are uncorrelated, which is almost never the case for real data (although it is very often true if the covariates are synthetic, as is the case for the [design matrix](https://en.wikipedia.org/wiki/Design_matrix) of well designed experiments). This is well known in some parts of the scientific community and ignored in others. \n",
    "\n",
    "This reasoning is also recapitulared in the [Haufe et al study](https://www.sciencedirect.com/science/article/pii/S1053811913010914) in which the authors show that the optimal generative model (explanation/interpretability model) $a\\in R^d$ for a linear binary classification model $w\\in R^d$ and $n$ $d$-dimensional data points stored in a matrix $X\\in R^{n\\times d}$ is\n",
    "\n",
    "$a = X^{\\top}Xw = X^{\\top}\\hat{y}$\n",
    "\n",
    "where $\\hat{y}$ are the predictions of the linear model and we assume that the data as well as the predictions are centered (meaning $\\sum_i^n x_i=0$) and have unit variance (meaning  $\\sum_i^n x_i^2=1$). Following Haufe et al. the model explanations are referred to as *pattern*.\n",
    "\n",
    "Note that as we are dealing with with very high-dimensional data in the case of bag-of-words featurized text data, we approximate this z-scoring operation in order to avoid densifying the sparse data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'worst', 'waste', 'awful', 'terrible']\n"
     ]
    }
   ],
   "source": [
    "labels_normalized = StandardScaler().fit_transform(predictions_proba)\n",
    "data_scaled = StandardScaler(with_mean=False).fit_transform(X_test)\n",
    "prediction_sign = -np.sign(predictions_proba.argmax(axis=1) - .5)\n",
    "\n",
    "pattern = labels_normalized[:,0].T @ data_scaled\n",
    "\n",
    "idx2word = {idx: word for word, idx in vect.vocabulary_.items()}\n",
    "\n",
    "# print most covarying words for negative sentiment class\n",
    "print([idx2word[idx] for idx in pattern.argsort()[-5:][::-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations for single data points / predictions\n",
    "\n",
    "The above model is a global interpretability model, meaning there is one pattern/explanation per class, but we would like to obtain explanations for single predictions. \n",
    "\n",
    "This can be done by computing the elementwise product between the feature vector of the $i$th data point and here the positive pattern $a^{pos}$:\n",
    "\n",
    "$a^{pos}_{i} = sign(\\hat{y}_i) a^{pos} \\circ x_i$\n",
    "\n",
    "where $\\circ$ stands for elementwise multiplication. We multiply by the $sign(\\hat{y}_i)$ to obtain positive values for the predicted class, independent of whether the predicted class was negative or positive sentiment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = sp.sparse.diags(prediction_sign) * ((data_scaled>0)*1.) @ sp.sparse.diags(pattern)\n",
    "\n",
    "sample = test_df.sample(n=100)\n",
    "sample['highlighted_features_covar'] = ''\n",
    "for i in range(len(sample)):\n",
    "    idx = sample.index[i]\n",
    "    top_words = word_scores[idx,:].toarray().flatten().argsort()[-top_what:][::-1]\n",
    "    words = [idx2word[w_idx] for w_idx in top_words]\n",
    "    sentence_covar = test_df.loc[idx, features]\n",
    "    for word in words:\n",
    "        sentence_covar = re.sub(r'\\b{}\\b'.format(word), \n",
    "                                '<mark>{}</mark>'.format(word),\n",
    "                                sentence_covar, \n",
    "                                flags=re.IGNORECASE\n",
    "                               )\n",
    "    sample.loc[idx, 'highlighted_features_covar'] = sentence_covar\n",
    "    sample.loc[idx, 'n_words_highlighted_in_sentence_covar'] = len(re.findall('<mark>', sentence_covar))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentiment neg, predicted sentiment neg (p=0.69)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "This is a tedious <mark>movie</mark>. The real villains are the clunky adaptation (it's embarrassingly easy to tell that the source material was a novel) and witless screenplay.On the credit side, considering the budget was tight due to wartime austerity, the look of the film isn't at all <mark>bad</mark>. And the performances are, by and large, OK, except for Phyllis Calvert, who is terrific - a miracle considering the potential for winsomeness, a pit into which she most definitely does not fall. Ms Calvert, with a lot less to go on, is as accomplished as Olivia de Havilland in Gone With The Wind.The one absolutely unbearable aspect of The Man in Grey is the dreadfully conceived depiction of a black serving boy. No matter that he's meant to be a sympathetic character. Played badly by a white boy in black-face <mark>make</mark>-up, it is impossible to by-pass this example of condescending racism.Grim."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "rand_example = sample.sample(n=1)\n",
    "print(f\"True sentiment {rand_example['rating_binary'].values[0]}, predicted sentiment {rand_example['predictions'].values[0]} (p={rand_example['prediction_proba_max'].values[0]:0.2})\")\n",
    "display(HTML(rand_example['highlighted_features_covar'].values[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('ppp': conda)",
   "language": "python",
   "name": "python361064bitpppconda08cb32434e5c43ab89b88a4949d5ebb2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
